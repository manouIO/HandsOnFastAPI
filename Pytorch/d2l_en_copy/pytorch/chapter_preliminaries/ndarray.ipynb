{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de623d7",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Data Manipulation\n",
    ":label:`sec_ndarray`\n",
    "\n",
    "In order to get anything done, \n",
    "we need some way to store and manipulate data.\n",
    "Generally, there are two important things \n",
    "we need to do with data: \n",
    "(i) acquire them; \n",
    "and (ii) process them once they are inside the computer. \n",
    "There is no point in acquiring data \n",
    "without some way to store it, \n",
    "so to start, let's get our hands dirty\n",
    "with $n$-dimensional arrays, \n",
    "which we also call *tensors*.\n",
    "If you already know the NumPy \n",
    "scientific computing package, \n",
    "this will be a breeze.\n",
    "For all modern deep learning frameworks,\n",
    "the *tensor class* (`ndarray` in MXNet, \n",
    "`Tensor` in PyTorch and TensorFlow) \n",
    "resembles NumPy's `ndarray`,\n",
    "with a few killer features added.\n",
    "First, the tensor class\n",
    "supports automatic differentiation.\n",
    "Second, it leverages GPUs\n",
    "to accelerate numerical computation,\n",
    "whereas NumPy only runs on CPUs.\n",
    "These properties make neural networks\n",
    "both easy to code and fast to run.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dc517",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "(**To start, we import the PyTorch library.\n",
    "Note that the package name is `torch`.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fa8e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:55.152236Z",
     "iopub.status.busy": "2023-08-18T19:32:55.151500Z",
     "iopub.status.idle": "2023-08-18T19:32:57.051589Z",
     "shell.execute_reply": "2023-08-18T19:32:57.050409Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d828de8",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "[**A tensor represents a (possibly multidimensional) array of numerical values.**]\n",
    "In the one-dimensional case, i.e., when only one axis is needed for the data,\n",
    "a tensor is called a *vector*.\n",
    "With two axes, a tensor is called a *matrix*.\n",
    "With $k > 2$ axes, we drop the specialized names\n",
    "and just refer to the object as a $k^\\textrm{th}$-*order tensor*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a471639",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "PyTorch provides a variety of functions \n",
    "for creating new tensors \n",
    "prepopulated with values. \n",
    "For example, by invoking `arange(n)`,\n",
    "we can create a vector of evenly spaced values,\n",
    "starting at 0 (included) \n",
    "and ending at `n` (not included).\n",
    "By default, the interval size is $1$.\n",
    "Unless otherwise specified, \n",
    "new tensors are stored in main memory \n",
    "and designated for CPU-based computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6aa30a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.056039Z",
     "iopub.status.busy": "2023-08-18T19:32:57.055276Z",
     "iopub.status.idle": "2023-08-18T19:32:57.089028Z",
     "shell.execute_reply": "2023-08-18T19:32:57.088195Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12b5d8",
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Each of these values is called\n",
    "an *element* of the tensor.\n",
    "The tensor `x` contains 12 elements.\n",
    "We can inspect the total number of elements \n",
    "in a tensor via its `numel` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640cadaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.093138Z",
     "iopub.status.busy": "2023-08-18T19:32:57.092473Z",
     "iopub.status.idle": "2023-08-18T19:32:57.098450Z",
     "shell.execute_reply": "2023-08-18T19:32:57.097452Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c7483",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "(**We can access a tensor's *shape***) \n",
    "(the length along each axis)\n",
    "by inspecting its `shape` attribute.\n",
    "Because we are dealing with a vector here,\n",
    "the `shape` contains just a single element\n",
    "and is identical to the size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0a9616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.102194Z",
     "iopub.status.busy": "2023-08-18T19:32:57.101575Z",
     "iopub.status.idle": "2023-08-18T19:32:57.107424Z",
     "shell.execute_reply": "2023-08-18T19:32:57.106501Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60413a",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "We can [**change the shape of a tensor\n",
    "without altering its size or values**],\n",
    "by invoking `reshape`.\n",
    "For example, we can transform \n",
    "our vector `x` whose shape is (12,) \n",
    "to a matrix `X`  with shape (3, 4).\n",
    "This new tensor retains all elements\n",
    "but reconfigures them into a matrix.\n",
    "Notice that the elements of our vector\n",
    "are laid out one row at a time and thus\n",
    "`x[3] == X[0, 3]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6092207c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.111467Z",
     "iopub.status.busy": "2023-08-18T19:32:57.110749Z",
     "iopub.status.idle": "2023-08-18T19:32:57.117759Z",
     "shell.execute_reply": "2023-08-18T19:32:57.116917Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e1706",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Note that specifying every shape component\n",
    "to `reshape` is redundant.\n",
    "Because we already know our tensor's size,\n",
    "we can work out one component of the shape given the rest.\n",
    "For example, given a tensor of size $n$\n",
    "and target shape ($h$, $w$),\n",
    "we know that $w = n/h$.\n",
    "To automatically infer one component of the shape,\n",
    "we can place a `-1` for the shape component\n",
    "that should be inferred automatically.\n",
    "In our case, instead of calling `x.reshape(3, 4)`,\n",
    "we could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`.\n",
    "\n",
    "Practitioners often need to work with tensors\n",
    "initialized to contain all 0s or 1s.\n",
    "[**We can construct a tensor with all elements set to 0**] (~~or one~~)\n",
    "and a shape of (2, 3, 4) via the `zeros` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383cafca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.122018Z",
     "iopub.status.busy": "2023-08-18T19:32:57.121194Z",
     "iopub.status.idle": "2023-08-18T19:32:57.128294Z",
     "shell.execute_reply": "2023-08-18T19:32:57.127285Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4115a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor=torch.tensor([[[1,2,3],\n",
    "                      [4,5,6],\n",
    "                      [2,4,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e21a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten1=torch.tensor([7]) #this is a vector with one component ndim=1\n",
    "ten2=torch.tensor(7) # this is a scalar ndim=0\n",
    "\n",
    "ten1.shape,ten2.shape\n",
    "ten1.ndim, ten2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33726ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e967d02",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "Similarly, we can create a tensor \n",
    "with all 1s by invoking `ones`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea249d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.132534Z",
     "iopub.status.busy": "2023-08-18T19:32:57.131716Z",
     "iopub.status.idle": "2023-08-18T19:32:57.139029Z",
     "shell.execute_reply": "2023-08-18T19:32:57.138135Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f2d6",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "We often wish to \n",
    "[**sample each element randomly (and independently)**] \n",
    "from a given probability distribution.\n",
    "For example, the parameters of neural networks\n",
    "are often initialized randomly.\n",
    "The following snippet creates a tensor \n",
    "with elements drawn from \n",
    "a standard Gaussian (normal) distribution\n",
    "with mean 0 and standard deviation 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24edc3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9531, 0.8242, 0.0789],\n",
       "         [0.7396, 0.8031, 0.1492],\n",
       "         [0.2380, 0.3564, 0.7343],\n",
       "         [0.5187, 0.6328, 0.3483]],\n",
       "\n",
       "        [[0.7116, 0.2065, 0.9926],\n",
       "         [0.7668, 0.4130, 0.2743],\n",
       "         [0.8024, 0.0662, 0.0535],\n",
       "         [0.5377, 0.3670, 0.2612]],\n",
       "\n",
       "        [[0.1419, 0.5728, 0.5454],\n",
       "         [0.2193, 0.7477, 0.4452],\n",
       "         [0.4584, 0.9817, 0.1367],\n",
       "         [0.7069, 0.3459, 0.0218]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 4,3) #generates a tensor of shape (3, 4, 3) filled with random numbers drawn from a uniform distribution over the interval [0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2254595d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.143051Z",
     "iopub.status.busy": "2023-08-18T19:32:57.142388Z",
     "iopub.status.idle": "2023-08-18T19:32:57.149695Z",
     "shell.execute_reply": "2023-08-18T19:32:57.148813Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7782,  1.6289, -0.4983],\n",
       "         [ 1.2196,  0.0628, -2.2105],\n",
       "         [-1.6888, -0.9638,  0.7809],\n",
       "         [ 0.4890,  1.4705, -2.1912]],\n",
       "\n",
       "        [[ 0.6799, -1.0303, -0.2609],\n",
       "         [-0.5099,  0.2627, -0.1294],\n",
       "         [-0.7342, -0.4030,  0.0502],\n",
       "         [-0.0370,  0.4250,  1.2296]],\n",
       "\n",
       "        [[ 0.4702,  1.3400,  0.0840],\n",
       "         [-0.8706,  0.6263, -1.0948],\n",
       "         [ 0.0257,  1.6698, -0.5459],\n",
       "         [ 1.4377,  2.0713,  1.9311]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4,3) #generates a tensor of shape (3, 4, 3) filled with random numbers drawn from a standard normal distribution (mean = 0, variance = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7edce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1772,  1.1057, -1.9606],\n",
       "         [ 0.2824,  0.0237, -1.1283],\n",
       "         [ 1.0563, -1.4019,  0.2008],\n",
       "         [-0.6055, -0.3607, -0.0088]],\n",
       "\n",
       "        [[-0.1260, -0.2408,  1.1548],\n",
       "         [-1.1804, -0.0575, -0.1808],\n",
       "         [-0.8598,  0.8117, -0.2064],\n",
       "         [ 0.2217,  1.5472, -0.6723]],\n",
       "\n",
       "        [[-1.0358,  0.3641, -0.2455],\n",
       "         [-0.5308, -1.0396, -0.0999],\n",
       "         [ 1.1846, -0.0992, -0.1019],\n",
       "         [ 2.3767,  0.0076,  1.2918]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(size=(3, 4,3)) #either way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eda39",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "Finally, we can construct tensors by\n",
    "[**supplying the exact values for each element**] \n",
    "by supplying (possibly nested) Python list(s) \n",
    "containing numerical literals.\n",
    "Here, we construct a matrix with a list of lists,\n",
    "where the outermost list corresponds to axis 0,\n",
    "and the inner list corresponds to axis 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26863d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.153567Z",
     "iopub.status.busy": "2023-08-18T19:32:57.153222Z",
     "iopub.status.idle": "2023-08-18T19:32:57.160436Z",
     "shell.execute_reply": "2023-08-18T19:32:57.159548Z"
    },
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b589cdb",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "As with  Python lists,\n",
    "we can access tensor elements \n",
    "by indexing (starting with 0).\n",
    "To access an element based on its position\n",
    "relative to the end of the list,\n",
    "we can use negative indexing.\n",
    "Finally, we can access whole ranges of indices \n",
    "via slicing (e.g., `X[start:stop]`), \n",
    "where the returned value includes \n",
    "the first index (`start`) *but not the last* (`stop`).\n",
    "Finally, when only one index (or slice)\n",
    "is specified for a $k^\\textrm{th}$-order tensor,\n",
    "it is applied along axis 0.\n",
    "Thus, in the following code,\n",
    "[**`[-1]` selects the last row and `[1:3]`\n",
    "selects the second and third rows**].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9049a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.164537Z",
     "iopub.status.busy": "2023-08-18T19:32:57.163812Z",
     "iopub.status.idle": "2023-08-18T19:32:57.171699Z",
     "shell.execute_reply": "2023-08-18T19:32:57.170451Z"
    },
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450673b",
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Beyond reading them, (**we can also *write* elements of a matrix by specifying indices.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9246619c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.176047Z",
     "iopub.status.busy": "2023-08-18T19:32:57.175685Z",
     "iopub.status.idle": "2023-08-18T19:32:57.182893Z",
     "shell.execute_reply": "2023-08-18T19:32:57.181890Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f06903",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "If we want [**to assign multiple elements the same value,\n",
    "we apply the indexing on the left-hand side \n",
    "of the assignment operation.**]\n",
    "For instance, `[:2, :]`  accesses \n",
    "the first and second rows,\n",
    "where `:` takes all the elements along axis 1 (column).\n",
    "While we discussed indexing for matrices,\n",
    "this also works for vectors\n",
    "and for tensors of more than two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0532f024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.186970Z",
     "iopub.status.busy": "2023-08-18T19:32:57.186270Z",
     "iopub.status.idle": "2023-08-18T19:32:57.193303Z",
     "shell.execute_reply": "2023-08-18T19:32:57.192338Z"
    },
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdce97",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "## Operations\n",
    "\n",
    "Now that we know how to construct tensors\n",
    "and how to read from and write to their elements,\n",
    "we can begin to manipulate them\n",
    "with various mathematical operations.\n",
    "Among the most useful of these \n",
    "are the *elementwise* operations.\n",
    "These apply a standard scalar operation\n",
    "to each element of a tensor.\n",
    "For functions that take two tensors as inputs,\n",
    "elementwise operations apply some standard binary operator\n",
    "on each pair of corresponding elements.\n",
    "We can create an elementwise function \n",
    "from any function that maps \n",
    "from a scalar to a scalar.\n",
    "\n",
    "In mathematical notation, we denote such\n",
    "*unary* scalar operators (taking one input)\n",
    "by the signature \n",
    "$f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "This just means that the function maps\n",
    "from any real number onto some other real number.\n",
    "Most standard operators, including unary ones like $e^x$, can be applied elementwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd6724c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.197301Z",
     "iopub.status.busy": "2023-08-18T19:32:57.196599Z",
     "iopub.status.idle": "2023-08-18T19:32:57.206136Z",
     "shell.execute_reply": "2023-08-18T19:32:57.205188Z"
    },
    "origin_pos": 61,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n",
       "        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n",
       "         22026.4648,  59874.1406])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f353f",
   "metadata": {
    "origin_pos": 64
   },
   "source": [
    "Likewise, we denote *binary* scalar operators,\n",
    "which map pairs of real numbers\n",
    "to a (single) real number\n",
    "via the signature \n",
    "$f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ \n",
    "and $\\mathbf{v}$ *of the same shape*,\n",
    "and a binary operator $f$, we can produce a vector\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
    "where $c_i, u_i$, and $v_i$ are the $i^\\textrm{th}$ elements\n",
    "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
    "Here, we produced the vector-valued\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "by *lifting* the scalar function\n",
    "to an elementwise vector operation.\n",
    "The common standard arithmetic operators\n",
    "for addition (`+`), subtraction (`-`), \n",
    "multiplication (`*`), division (`/`), \n",
    "and exponentiation (`**`)\n",
    "have all been *lifted* to elementwise operations\n",
    "for identically-shaped tensors of arbitrary shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89bc996d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.210417Z",
     "iopub.status.busy": "2023-08-18T19:32:57.209741Z",
     "iopub.status.idle": "2023-08-18T19:32:57.219298Z",
     "shell.execute_reply": "2023-08-18T19:32:57.218318Z"
    },
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y #x*y element-wise multiplication, if you want matrice multiplication x.shape=(n,m) and y.shape=(m,p), use torch.matmul (x,y) or x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79b3099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype,y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3d16704",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "torch.mm(x,y.type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3553a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype #to inquiry the datatype, unless you specify another, the default is float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b4e952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  30.,   33.,   36.,   39.,   42.,   45.],\n",
       "         [  84.,   96.,  108.,  120.,  132.,  144.],\n",
       "         [ 138.,  159.,  180.,  201.,  222.,  243.],\n",
       "         [ 192.,  222.,  252.,  282.,  312.,  342.],\n",
       "         [ 246.,  285.,  324.,  363.,  402.,  441.]],\n",
       "\n",
       "        [[ 300.,  348.,  396.,  444.,  492.,  540.],\n",
       "         [ 354.,  411.,  468.,  525.,  582.,  639.],\n",
       "         [ 408.,  474.,  540.,  606.,  672.,  738.],\n",
       "         [ 462.,  537.,  612.,  687.,  762.,  837.],\n",
       "         [ 516.,  600.,  684.,  768.,  852.,  936.]],\n",
       "\n",
       "        [[ 570.,  663.,  756.,  849.,  942., 1035.],\n",
       "         [ 624.,  726.,  828.,  930., 1032., 1134.],\n",
       "         [ 678.,  789.,  900., 1011., 1122., 1233.],\n",
       "         [ 732.,  852.,  972., 1092., 1212., 1332.],\n",
       "         [ 786.,  915., 1044., 1173., 1302., 1431.]],\n",
       "\n",
       "        [[ 840.,  978., 1116., 1254., 1392., 1530.],\n",
       "         [ 894., 1041., 1188., 1335., 1482., 1629.],\n",
       "         [ 948., 1104., 1260., 1416., 1572., 1728.],\n",
       "         [1002., 1167., 1332., 1497., 1662., 1827.],\n",
       "         [1056., 1230., 1404., 1578., 1752., 1926.]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_first_tens  = torch.arange(60. , dtype=torch.float32).reshape(4, 5,3)\n",
    "my_second_tens = torch.arange(18. ,  dtype=torch.float32).reshape(3,6)\n",
    "\n",
    "\n",
    "result = torch.matmul(my_first_tens, my_second_tens)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6eaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee86875f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 6])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae1d38",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "400ca23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_to_thousand=torch.arange(0,1001,50)\n",
    "zero_to_thousand\n",
    "zero_to_thousand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdb844a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c7b27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_like=torch.zeros_like(zero_to_thousand)\n",
    "zeros_like.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314792a",
   "metadata": {},
   "source": [
    "### Tensor datatypes\n",
    "\n",
    "**Note:** Tensor datatypes is one of the 3 big errors you' ll run into with PyTorch\n",
    "1. Tensor not right datatype\n",
    "2. Tensor not right shape\n",
    "3. Tensor not on the right device (for example if one tensor lives on cpu, and the other on gpu, and you try some operations, you might run into errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7661aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97c1238",
   "metadata": {},
   "source": [
    "In addition to elementwise computations,\n",
    "we can also perform linear algebraic operations,\n",
    "such as dot products and matrix multiplications.\n",
    "We will elaborate on these\n",
    "in :numref:`sec_linear-algebra`.\n",
    "\n",
    "We can also [***concatenate* multiple tensors,**]\n",
    "stacking them end-to-end to form a larger one.\n",
    "We just need to provide a list of tensors\n",
    "and tell the system along which axis to concatenate.\n",
    "The example below shows what happens when we concatenate\n",
    "two matrices along rows (axis 0)\n",
    "instead of columns (axis 1).\n",
    "We can see that the first output's axis-0 length ($6$)\n",
    "is the sum of the two input tensors' axis-0 lengths ($3 + 3$);\n",
    "while the second output's axis-1 length ($8$)\n",
    "is the sum of the two input tensors' axis-1 lengths ($4 + 4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "43aa9012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.223534Z",
     "iopub.status.busy": "2023-08-18T19:32:57.222711Z",
     "iopub.status.idle": "2023-08-18T19:32:57.233166Z",
     "shell.execute_reply": "2023-08-18T19:32:57.232145Z"
    },
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4)) #you can change the datatype to float16, float64, \n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "09cd55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z=torch.cat((X, Y), dim=0) #cat is different fron stack\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d094b84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7877bc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_float16= X.type(torch.float16)\n",
    "x_float16\n",
    "x_float16.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e867b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*x_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de41548",
   "metadata": {},
   "source": [
    "### To get information from a tensor (tensor attributes)\n",
    "`x.device`\n",
    "\n",
    "`x.dtype`\n",
    "\n",
    "`x.shape`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed8dd5",
   "metadata": {},
   "source": [
    "### Tensor multiplication\n",
    "#### element wise\n",
    "#### Matrix multiplication (dot wise) (n,m) dot (m,p)= (n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e5b7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1=torch.tensor([1,2,3])\n",
    "vec2=torch.tensor([2,3,4])\n",
    "vec3=vec1*vec2 #this is element wise multiplication\n",
    "vec4=torch.matmul(vec1,vec2) # this is matrix multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48bd97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  6, 12])\n",
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "print(vec3), \n",
    "print(vec4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d587c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0107, -0.1403,  0.5225,  0.2271,  0.4562],\n",
      "        [-1.5123, -0.4007, -1.0209,  2.9739, -1.4645],\n",
      "        [-0.8624, -4.1877, -0.2863,  2.6682,  5.3995]])\n",
      "tensor([[ 0.0107, -0.1403,  0.5225,  0.2271,  0.4562],\n",
      "        [-1.5123, -0.4007, -1.0209,  2.9739, -1.4645],\n",
      "        [-0.8624, -4.1877, -0.2863,  2.6682,  5.3995]])\n",
      "CPU times: user 4.7 ms, sys: 67 Âµs, total: 4.76 ms\n",
      "Wall time: 4.06 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matrix1=torch.randn(3,4)\n",
    "matrix2=torch.randn(4,5)\n",
    "matrix3=torch.matmul(matrix1, matrix2) #this will run without error, because the inner dimensions match 4==4\n",
    "matrix31= matrix1 @ matrix2 # either way\n",
    "\n",
    "print(matrix3)\n",
    "print(matrix31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de94ee15",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x5 and 3x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matrix4\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#this will return an error, because the inner dimension does not match 5 != 3\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x5 and 3x4)"
     ]
    }
   ],
   "source": [
    "matrix4=torch.matmul(matrix2, matrix1) #this will return an error, because the inner dimension does not match 5 != 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb3ec4",
   "metadata": {},
   "source": [
    "### Finding Min, Max, Mean, Sum, etc... (Tensor aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08c6f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])\n",
      "min = tensor(1)\n",
      "max= tensor(91)\n"
     ]
    }
   ],
   "source": [
    "X=torch.arange(1,101,10)\n",
    "print(X)\n",
    "#min\n",
    "print( 'min =' ,torch.min(X))\n",
    "#max\n",
    "print ('max=', torch.max(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b41eff85",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#torch.mean(X.type(torch.long)) # mean function can't work with long datatype\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "#torch.mean(X.type(torch.long)) # mean function can't work with long datatype\n",
    "torch.mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f0c63",
   "metadata": {},
   "source": [
    "### FInding the position of Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c662ab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FInd the position of the min of X\n",
    "torch.argmin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "237ed463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the position of the max of X\n",
    "torch.argmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5578b",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, squeezing and unsqueezing tensors\n",
    "**Reshaping**- reshapes an input tensor to a defined shape\n",
    "\n",
    "**View** - return a view of an input tensor `x` of a certain shape and keep the same memory as the original tensor,  creates a new tensor `x_view` that shares the same underlying memory as the original tensor x. This means:\n",
    "\n",
    "- Both `x` and `x_view` point to the same data in memory.\n",
    "\n",
    "- If you change a value in `x_view`, the change will be reflected in x, and vice versa.\n",
    "\n",
    "**Stacking** -Combine multiple tensors on top of each other (vstack) or side by side (hstack)\n",
    "\n",
    "**Squeezes** -removes all `1` dimensions from a tensor\n",
    "\n",
    "**Unsqueeze** -add a `1` dimension to a target tensor\n",
    "\n",
    "**Permute** -Return a view of the input with dimensions permuted (swapped) in a certain way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a71ae1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "         55, 58]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(1,60,3)\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a78ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4,  7, 10],\n",
       "         [13, 16, 19, 22],\n",
       "         [25, 28, 31, 34],\n",
       "         [37, 40, 43, 46],\n",
       "         [49, 52, 55, 58]]),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped=x.reshape(5,4) ## the shape must be compatible with he original size ie reshape(m,n) when m*n=original shape\n",
    "x_reshaped, x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1b72659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4,  7, 10, 13],\n",
       "         [16, 19, 22, 25, 28],\n",
       "         [31, 34, 37, 40, 43],\n",
       "         [46, 49, 52, 55, 58]]),\n",
       " torch.Size([4, 5]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the view\n",
    "x_view=x.view(4,5) # x_view chares the memory with the original tensor x, that means changing x_view also changes x\n",
    "x_view , x_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b25794e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5,  4,  7, 10, 13],\n",
       "         [16, 19, 22, 25, 28],\n",
       "         [31, 34, 37, 40, 43],\n",
       "         [46, 49, 52, 55, 58]]),\n",
       " tensor([ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "         55, 58]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_view[0,0]=5\n",
    "x_view,x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5efe4",
   "metadata": {},
   "source": [
    "If you want a new tensor that does not share memory with `x`, use `x.clone` or `x.detach.clone()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d2482ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5,  4,  7,  9, 13],\n",
       "         [16, 19, 22, 25, 28],\n",
       "         [31, 34, 37, 40, 43],\n",
       "         [46, 49, 52, 55, 58]]),\n",
       " tensor([ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "         55, 58]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_clone=x.clone()\n",
    "x_clone_view=x_clone.view(4,5)\n",
    "x_clone_view[0,3]=9\n",
    "x_clone_view, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "26da6033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "          55, 58],\n",
       "         [ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "          55, 58],\n",
       "         [ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "          55, 58],\n",
       "         [ 5,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49, 52,\n",
       "          55, 58]]),\n",
       " torch.Size([4, 20]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked=torch.stack([x,x,x,x],dim=0)\n",
    "x_stacked, x_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49b48ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_reshaped shape: torch.Size([3, 9])\n",
      "b_reshaped shape torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor 'a' from 20 to 98 stepping by 3\n",
    "a = torch.arange(20, 100, 3)\n",
    "\n",
    "# Reshape 'a' to shape (3, 9)\n",
    "a_reshaped = a.reshape(3, 9)\n",
    "\n",
    "# Create another tensor 'b' of shape (3, 9), \n",
    "b = torch.linspace(100,178,27)\n",
    "b_reshaped=b.reshape(3,9)\n",
    "\n",
    "print('a_reshaped shape:', a_reshaped.shape)\n",
    "print('b_reshaped shape', b_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "135f85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  23.,  26.,  29.,  32.,  35.,  38.,  41.,  44.],\n",
      "         [ 47.,  50.,  53.,  56.,  59.,  62.,  65.,  68.,  71.],\n",
      "         [ 74.,  77.,  80.,  83.,  86.,  89.,  92.,  95.,  98.]],\n",
      "\n",
      "        [[100., 103., 106., 109., 112., 115., 118., 121., 124.],\n",
      "         [127., 130., 133., 136., 139., 142., 145., 148., 151.],\n",
      "         [154., 157., 160., 163., 166., 169., 172., 175., 178.]]]) torch.Size([2, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "stack_dim0 = torch.stack([a_reshaped, b_reshaped], dim=0)\n",
    "print(stack_dim0,stack_dim0.shape)  # torch.Size([2, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3c8180c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  23.,  26.,  29.,  32.,  35.,  38.,  41.,  44.],\n",
      "         [100., 103., 106., 109., 112., 115., 118., 121., 124.]],\n",
      "\n",
      "        [[ 47.,  50.,  53.,  56.,  59.,  62.,  65.,  68.,  71.],\n",
      "         [127., 130., 133., 136., 139., 142., 145., 148., 151.]],\n",
      "\n",
      "        [[ 74.,  77.,  80.,  83.,  86.,  89.,  92.,  95.,  98.],\n",
      "         [154., 157., 160., 163., 166., 169., 172., 175., 178.]]]) torch.Size([3, 2, 9])\n"
     ]
    }
   ],
   "source": [
    "stack_dim0 = torch.stack([a_reshaped, b_reshaped], dim=1)\n",
    "print(stack_dim0,stack_dim0.shape)  # torch.Size([3, 2, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a850034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20., 100.],\n",
      "         [ 23., 103.],\n",
      "         [ 26., 106.],\n",
      "         [ 29., 109.],\n",
      "         [ 32., 112.],\n",
      "         [ 35., 115.],\n",
      "         [ 38., 118.],\n",
      "         [ 41., 121.],\n",
      "         [ 44., 124.]],\n",
      "\n",
      "        [[ 47., 127.],\n",
      "         [ 50., 130.],\n",
      "         [ 53., 133.],\n",
      "         [ 56., 136.],\n",
      "         [ 59., 139.],\n",
      "         [ 62., 142.],\n",
      "         [ 65., 145.],\n",
      "         [ 68., 148.],\n",
      "         [ 71., 151.]],\n",
      "\n",
      "        [[ 74., 154.],\n",
      "         [ 77., 157.],\n",
      "         [ 80., 160.],\n",
      "         [ 83., 163.],\n",
      "         [ 86., 166.],\n",
      "         [ 89., 169.],\n",
      "         [ 92., 172.],\n",
      "         [ 95., 175.],\n",
      "         [ 98., 178.]]]) torch.Size([3, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "stack_dim2= torch.stack([a_reshaped, b_reshaped], dim=2)\n",
    "print(stack_dim2,stack_dim2.shape)  # torch.Size([3, 9, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e524944",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stack_dim3\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_reshaped\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(stack_dim0,stack_dim0\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# error message because 3 is out of range of ndim==2 for a_reshaped and b_reshaped\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "stack_dim3= torch.stack([a_reshaped, b_reshaped], dim=3)\n",
    "print(stack_dim0,stack_dim0.shape)  # error message because 3 is out of range of ndim==2 for a_reshaped and b_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "def7389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_reshaped shape: torch.Size([4, 5])\n",
      "d_reshaped shape torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "##Another example\n",
    "\n",
    "# Create a tensor 'a' from 20 to 98 stepping by 3\n",
    "c = torch.arange(20, 120, 5)\n",
    "c_reshaped = c.reshape(4, 5)\n",
    "\n",
    "\n",
    "d = torch.linspace(121,140,20)\n",
    "d_reshaped=d.reshape(4,5)\n",
    "\n",
    "print('c_reshaped shape:', c_reshaped.shape)\n",
    "print('d_reshaped shape', d_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6cc44a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 20,  25,  30,  35,  40],\n",
       "         [ 45,  50,  55,  60,  65],\n",
       "         [ 70,  75,  80,  85,  90],\n",
       "         [ 95, 100, 105, 110, 115]]),\n",
       " tensor([[121., 122., 123., 124., 125.],\n",
       "         [126., 127., 128., 129., 130.],\n",
       "         [131., 132., 133., 134., 135.],\n",
       "         [136., 137., 138., 139., 140.]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_reshaped,d_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c709c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  25.,  30.,  35.,  40.],\n",
      "         [ 45.,  50.,  55.,  60.,  65.],\n",
      "         [ 70.,  75.,  80.,  85.,  90.],\n",
      "         [ 95., 100., 105., 110., 115.]],\n",
      "\n",
      "        [[121., 122., 123., 124., 125.],\n",
      "         [126., 127., 128., 129., 130.],\n",
      "         [131., 132., 133., 134., 135.],\n",
      "         [136., 137., 138., 139., 140.]]]) torch.Size([2, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "stack_dim0 = torch.stack([c_reshaped, d_reshaped], dim=0)\n",
    "print(stack_dim0,stack_dim0.shape)  # torch.Size([2, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "009a36f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  25.,  30.,  35.,  40.],\n",
      "         [121., 122., 123., 124., 125.]],\n",
      "\n",
      "        [[ 45.,  50.,  55.,  60.,  65.],\n",
      "         [126., 127., 128., 129., 130.]],\n",
      "\n",
      "        [[ 70.,  75.,  80.,  85.,  90.],\n",
      "         [131., 132., 133., 134., 135.]],\n",
      "\n",
      "        [[ 95., 100., 105., 110., 115.],\n",
      "         [136., 137., 138., 139., 140.]]]) torch.Size([4, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "stack_dim1= torch.stack([c_reshaped, d_reshaped], dim=1)\n",
    "print(stack_dim1,stack_dim1.shape)  # torch.Size([4, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "32cee7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20., 121.],\n",
      "         [ 25., 122.],\n",
      "         [ 30., 123.],\n",
      "         [ 35., 124.],\n",
      "         [ 40., 125.]],\n",
      "\n",
      "        [[ 45., 126.],\n",
      "         [ 50., 127.],\n",
      "         [ 55., 128.],\n",
      "         [ 60., 129.],\n",
      "         [ 65., 130.]],\n",
      "\n",
      "        [[ 70., 131.],\n",
      "         [ 75., 132.],\n",
      "         [ 80., 133.],\n",
      "         [ 85., 134.],\n",
      "         [ 90., 135.]],\n",
      "\n",
      "        [[ 95., 136.],\n",
      "         [100., 137.],\n",
      "         [105., 138.],\n",
      "         [110., 139.],\n",
      "         [115., 140.]]]) torch.Size([4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "stack_dim2= torch.stack([c_reshaped, d_reshaped], dim=2)\n",
    "print(stack_dim2,stack_dim2.shape)  # torch.Size([4, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5ba95b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 20.,  25.,  30.,  35.,  40., 121., 122., 123., 124., 125.],\n",
      "        [ 45.,  50.,  55.,  60.,  65., 126., 127., 128., 129., 130.],\n",
      "        [ 70.,  75.,  80.,  85.,  90., 131., 132., 133., 134., 135.],\n",
      "        [ 95., 100., 105., 110., 115., 136., 137., 138., 139., 140.]]) torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "cat0= torch.cat([c_reshaped, d_reshaped], dim=1)\n",
    "print(cat0,cat0.shape)  # torch.Size([4, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "697cb451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_reshaped.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n",
       " tensor([[0., 1., 2., 3., 4.],\n",
       "         [5., 6., 7., 8., 9.]]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vec1 =  torch.arange(10.)\n",
    "my_vec2 =  my_vec1.view(2,5)\n",
    "my_vec1,my_vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-100.,    1.,    2.,    3.,    4.,    5.,    6.,    7.,    8.,    9.]),\n",
       " tensor([[-100.,    1.,    2.,    3.,    4.],\n",
       "         [   5.,    6.,    7.,    8.,    9.]]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vec1[0]= -100\n",
    "my_vec1 , my_vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416200c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vec1.view(2,5) , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_second_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "03629cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_reshaped shape: torch.Size([3, 5])\n",
      "f_reshaped shape torch.Size([3, 5])\n",
      "g_reshaped shape torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "##Another example\n",
    "\n",
    "e = torch.arange(20, 95, 5)\n",
    "e_reshaped = e.reshape(3, 5)\n",
    "\n",
    "\n",
    "f = torch.linspace(121,135,15)\n",
    "f_reshaped=f.reshape(3,5)\n",
    "\n",
    "g = torch.arange(150,225,5)\n",
    "g_reshaped=g.reshape(3,5)\n",
    "\n",
    "\n",
    "\n",
    "print('e_reshaped shape:', e_reshaped.shape)\n",
    "print('f_reshaped shape', f_reshaped.shape)\n",
    "print('g_reshaped shape', g_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3196b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[20, 25, 30, 35, 40],\n",
       "         [45, 50, 55, 60, 65],\n",
       "         [70, 75, 80, 85, 90]]),\n",
       " tensor([[121., 122., 123., 124., 125.],\n",
       "         [126., 127., 128., 129., 130.],\n",
       "         [131., 132., 133., 134., 135.]]),\n",
       " tensor([[150, 155, 160, 165, 170],\n",
       "         [175, 180, 185, 190, 195],\n",
       "         [200, 205, 210, 215, 220]]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_reshaped,f_reshaped,g_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d80abd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  25.,  30.,  35.,  40.],\n",
      "         [ 45.,  50.,  55.,  60.,  65.],\n",
      "         [ 70.,  75.,  80.,  85.,  90.]],\n",
      "\n",
      "        [[121., 122., 123., 124., 125.],\n",
      "         [126., 127., 128., 129., 130.],\n",
      "         [131., 132., 133., 134., 135.]],\n",
      "\n",
      "        [[150., 155., 160., 165., 170.],\n",
      "         [175., 180., 185., 190., 195.],\n",
      "         [200., 205., 210., 215., 220.]]]) torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "stack_dim0= torch.stack([e_reshaped, f_reshaped,g_reshaped], dim=0)\n",
    "print(stack_dim0,stack_dim0.shape)  # torch.Size([3, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "386e282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20.,  25.,  30.,  35.,  40.],\n",
      "         [121., 122., 123., 124., 125.],\n",
      "         [150., 155., 160., 165., 170.]],\n",
      "\n",
      "        [[ 45.,  50.,  55.,  60.,  65.],\n",
      "         [126., 127., 128., 129., 130.],\n",
      "         [175., 180., 185., 190., 195.]],\n",
      "\n",
      "        [[ 70.,  75.,  80.,  85.,  90.],\n",
      "         [131., 132., 133., 134., 135.],\n",
      "         [200., 205., 210., 215., 220.]]]) torch.Size([3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "stack_dim1= torch.stack([e_reshaped, f_reshaped,g_reshaped], dim=1)\n",
    "print(stack_dim1,stack_dim1.shape)  # torch.Size([3, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c3026078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 20., 121., 150.],\n",
      "         [ 25., 122., 155.],\n",
      "         [ 30., 123., 160.],\n",
      "         [ 35., 124., 165.],\n",
      "         [ 40., 125., 170.]],\n",
      "\n",
      "        [[ 45., 126., 175.],\n",
      "         [ 50., 127., 180.],\n",
      "         [ 55., 128., 185.],\n",
      "         [ 60., 129., 190.],\n",
      "         [ 65., 130., 195.]],\n",
      "\n",
      "        [[ 70., 131., 200.],\n",
      "         [ 75., 132., 205.],\n",
      "         [ 80., 133., 210.],\n",
      "         [ 85., 134., 215.],\n",
      "         [ 90., 135., 220.]]]) torch.Size([3, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "stack_dim2= torch.stack([e_reshaped, f_reshaped,g_reshaped], dim=2)\n",
    "print(stack_dim2,stack_dim2.shape)  # torch.Size([3, 5, 3]) ##ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9369a",
   "metadata": {},
   "source": [
    "### Squeeze (remove all single dimensions from a tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d2fdc3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape is torch.Size([1, 1, 10])\n",
      "x_squeeze().shape is torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[[4,5,3,6,7,1,2,5,8,9]]])\n",
    "print('x.shape is' , x.shape )           \n",
    "x_squeeze=x.squeeze() \n",
    "print ('x_squeeze().shape is', x.squeeze().shape) #all the 1 have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ba85f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_squeeze_unsqueeze shape is torch.Size([10, 1])\n",
      "x_squeeze_unsqueeze tensor([[4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze() adds a single dimension to a target tensor\n",
    "x_squeeze_unsqueeze=x_squeeze.unsqueeze(dim=1) # dim=0 add the zero th dim, dim=1 add the second dim\n",
    "print ('x_squeeze_unsqueeze shape is', x_squeeze_unsqueeze.shape) \n",
    "print('x_squeeze_unsqueeze', x_squeeze_unsqueeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7b404358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.permute -rearranges the dimension of a target order and returns a view of the tensor (remenber view shares memory with original tensor)\n",
    "x=torch.rand(size=(224,224,3)) #[height, Width,colour_channels]\n",
    "x.size()\n",
    "#permute x to rearrange the axis (or dim) order\n",
    "x_permuted=torch.permute(x,dims=(2,0,1)) #or x.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "90f84d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous shape:torch.Size([224, 224, 3])\n",
      "New shape:torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(f\"previous shape:{x.shape}\")\n",
    "print(f\"New shape:{x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8d4b9beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(100.), tensor(100.))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing x_permutes also changes x\n",
    "x_permuted[0,10,11]=100\n",
    "x_permuted[0,10,11],x[10,11,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346adeed",
   "metadata": {
    "origin_pos": 74
   },
   "source": [
    "Sometimes, we want to \n",
    "[**construct a binary tensor via *logical statements*.**]\n",
    "Take `X == Y` as an example.\n",
    "For each position `i, j`, if `X[i, j]` and `Y[i, j]` are equal, \n",
    "then the corresponding entry in the result takes value `1`,\n",
    "otherwise it takes value `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d39e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.237276Z",
     "iopub.status.busy": "2023-08-18T19:32:57.236485Z",
     "iopub.status.idle": "2023-08-18T19:32:57.243133Z",
     "shell.execute_reply": "2023-08-18T19:32:57.242117Z"
    },
    "origin_pos": 75,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00448db5",
   "metadata": {
    "origin_pos": 76
   },
   "source": [
    "[**Summing all the elements in the tensor**] yields a tensor with only one element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080b0125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.247142Z",
     "iopub.status.busy": "2023-08-18T19:32:57.246480Z",
     "iopub.status.idle": "2023-08-18T19:32:57.253117Z",
     "shell.execute_reply": "2023-08-18T19:32:57.252212Z"
    },
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a78360",
   "metadata": {
    "origin_pos": 79
   },
   "source": [
    "## Broadcasting\n",
    ":label:`subsec_broadcasting`\n",
    "\n",
    "By now, you know how to perform \n",
    "elementwise binary operations\n",
    "on two tensors of the same shape. \n",
    "Under certain conditions,\n",
    "even when shapes differ, \n",
    "we can still [**perform elementwise binary operations\n",
    "by invoking the *broadcasting mechanism*.**]\n",
    "Broadcasting works according to \n",
    "the following two-step procedure:\n",
    "(i) expand one or both arrays\n",
    "by copying elements along axes with length 1\n",
    "so that after this transformation,\n",
    "the two tensors have the same shape;\n",
    "(ii) perform an elementwise operation\n",
    "on the resulting arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "be37d2de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.256932Z",
     "iopub.status.busy": "2023-08-18T19:32:57.256264Z",
     "iopub.status.idle": "2023-08-18T19:32:57.263823Z",
     "shell.execute_reply": "2023-08-18T19:32:57.262881Z"
    },
    "origin_pos": 81,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e8410",
   "metadata": {
    "origin_pos": 84
   },
   "source": [
    "Since `a` and `b` are $3\\times1$ \n",
    "and $1\\times2$ matrices, respectively,\n",
    "their shapes do not match up.\n",
    "Broadcasting produces a larger $3\\times2$ matrix \n",
    "by replicating matrix `a` along the columns\n",
    "and matrix `b` along the rows\n",
    "before adding them elementwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9f62e827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.267856Z",
     "iopub.status.busy": "2023-08-18T19:32:57.267172Z",
     "iopub.status.idle": "2023-08-18T19:32:57.273497Z",
     "shell.execute_reply": "2023-08-18T19:32:57.272587Z"
    },
    "origin_pos": 85,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]]),\n",
       " tensor([[0, 0],\n",
       "         [0, 1],\n",
       "         [0, 2]]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b,a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fc2eb",
   "metadata": {},
   "source": [
    "### Indexing( select data from tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "73584f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 20.,  25.,  30.,  35.,  40.],\n",
       "          [ 45.,  50.,  55.,  60.,  65.],\n",
       "          [ 70.,  75.,  80.,  85.,  90.]],\n",
       " \n",
       "         [[121., 122., 123., 124., 125.],\n",
       "          [126., 127., 128., 129., 130.],\n",
       "          [131., 132., 133., 134., 135.]],\n",
       " \n",
       "         [[150., 155., 160., 165., 170.],\n",
       "          [175., 180., 185., 190., 195.],\n",
       "          [200., 205., 210., 215., 220.]]]),\n",
       " torch.Size([3, 3, 5]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_dim0, stack_dim0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0b0658b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(127.), tensor(127.))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_dim0[1,1,1],stack_dim0[1][1][1]# second value of the 0th dimension, 1 index value(2nd value) of the first and 2nd dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5e8eaa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([121., 126., 131.])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_dim0[:,0,0],# all matrices first row, first column\n",
    "stack_dim0[:][0][0], #stack_dim0[:] returns the whole tensor unchanged. this means all the matrices, first matrix, first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3780567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([121., 122., 123., 124., 125.]),\n",
       " tensor([121., 122., 123., 124., 125.]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_dim0[1][:][0], stack_dim0[1,0,:] #it is equivalent to stack_dim0[1,0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d1b43e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([121., 126., 131.])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_dim0[1,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d68609",
   "metadata": {
    "origin_pos": 86
   },
   "source": [
    "## Saving Memory\n",
    "\n",
    "[**Running operations can cause new memory to be\n",
    "allocated to host results.**]\n",
    "For example, if we write `Y = X + Y`,\n",
    "we dereference the tensor that `Y` used to point to\n",
    "and instead point `Y` at the newly allocated memory.\n",
    "We can demonstrate this issue with Python's `id()` function,\n",
    "which gives us the exact address \n",
    "of the referenced object in memory.\n",
    "Note that after we run `Y = Y + X`,\n",
    "`id(Y)` points to a different location.\n",
    "That is because Python first evaluates `Y + X`,\n",
    "allocating new memory for the result \n",
    "and then points `Y` to this new location in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "754a7433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.277697Z",
     "iopub.status.busy": "2023-08-18T19:32:57.277047Z",
     "iopub.status.idle": "2023-08-18T19:32:57.283549Z",
     "shell.execute_reply": "2023-08-18T19:32:57.282613Z"
    },
    "origin_pos": 87,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d26f5",
   "metadata": {
    "origin_pos": 88
   },
   "source": [
    "This might be undesirable for two reasons.\n",
    "First, we do not want to run around\n",
    "allocating memory unnecessarily all the time.\n",
    "In machine learning, we often have\n",
    "hundreds of megabytes of parameters\n",
    "and update all of them multiple times per second.\n",
    "Whenever possible, we want to perform these updates *in place*.\n",
    "Second, we might point at the \n",
    "same parameters from multiple variables.\n",
    "If we do not update in place, \n",
    "we must be careful to update all of these references,\n",
    "lest we spring a memory leak \n",
    "or inadvertently refer to stale parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82880947",
   "metadata": {
    "origin_pos": 89,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Fortunately, (**performing in-place operations**) is easy.\n",
    "We can assign the result of an operation\n",
    "to a previously allocated array `Y`\n",
    "by using slice notation: `Y[:] = <expression>`.\n",
    "To illustrate this concept, \n",
    "we overwrite the values of tensor `Z`,\n",
    "after initializing it, using `zeros_like`,\n",
    "to have the same shape as `Y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d62609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.287695Z",
     "iopub.status.busy": "2023-08-18T19:32:57.286964Z",
     "iopub.status.idle": "2023-08-18T19:32:57.293078Z",
     "shell.execute_reply": "2023-08-18T19:32:57.292048Z"
    },
    "origin_pos": 92,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140381179266448\n",
      "id(Z): 140381179266448\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745b125",
   "metadata": {
    "origin_pos": 95,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**If the value of `X` is not reused in subsequent computations,\n",
    "we can also use `X[:] = X + Y` or `X += Y`\n",
    "to reduce the memory overhead of the operation.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c13447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.296911Z",
     "iopub.status.busy": "2023-08-18T19:32:57.296361Z",
     "iopub.status.idle": "2023-08-18T19:32:57.302754Z",
     "shell.execute_reply": "2023-08-18T19:32:57.301805Z"
    },
    "origin_pos": 97,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f887dd",
   "metadata": {
    "origin_pos": 99
   },
   "source": [
    "## Conversion to Other Python Objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd057d04",
   "metadata": {
    "origin_pos": 101,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\n",
    "The torch tensor and NumPy array \n",
    "will share their underlying memory, \n",
    "and changing one through an in-place operation \n",
    "will also change the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "576963aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.306812Z",
     "iopub.status.busy": "2023-08-18T19:32:57.306088Z",
     "iopub.status.idle": "2023-08-18T19:32:57.312356Z",
     "shell.execute_reply": "2023-08-18T19:32:57.311478Z"
    },
    "origin_pos": 103,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor, torch.float32, dtype('float32'))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numpy to Tensor\n",
    "A = X.numpy()\n",
    "B = torch.from_numpy(A) #warning: when converting from numpy -> pytorch, it reflect float64 unless specified otherwise by .type(torch.float32)\n",
    "type(A), type(B),B.dtype, A.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "984df6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=float32),\n",
       " numpy.ndarray)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=B.numpy() #and If we want to change a tensor into a numpy object\n",
    "C,type( C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f36bbf",
   "metadata": {},
   "source": [
    "### numpy object and tensor does not share memories, this means changing C wont affect B; and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2def017",
   "metadata": {
    "origin_pos": 106
   },
   "source": [
    "To (**convert a size-1 tensor to a Python scalar**),\n",
    "we can invoke the `item` function or Python's built-in functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "388c5252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:32:57.316471Z",
     "iopub.status.busy": "2023-08-18T19:32:57.315825Z",
     "iopub.status.idle": "2023-08-18T19:32:57.322867Z",
     "shell.execute_reply": "2023-08-18T19:32:57.322007Z"
    },
    "origin_pos": 108,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da934378",
   "metadata": {},
   "source": [
    "## Random seed (Reproducibility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8489e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "Random_seed=42\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_c=torch.rand(3,4)\n",
    "\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_d=torch.rand(3,4)\n",
    "\n",
    "print(random_tensor_c)\n",
    "print(random_tensor_d)\n",
    "print(random_tensor_c==random_tensor_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "807d5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1869, 0.9613, 0.6834, 0.8988, 0.0505],\n",
      "        [0.5555, 0.7861, 0.0566, 0.7842, 0.1480],\n",
      "        [0.0388, 0.1037, 0.4216, 0.2373, 0.8111]])\n",
      "tensor([[0.1869, 0.9613, 0.6834, 0.8988, 0.0505],\n",
      "        [0.5555, 0.7861, 0.0566, 0.7842, 0.1480],\n",
      "        [0.0388, 0.1037, 0.4216, 0.2373, 0.8111]])\n",
      "tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True],\n",
      "        [True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "Random_seed=45\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_c=torch.rand(3,5)\n",
    "\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_d=torch.rand(3,5)\n",
    "\n",
    "print(random_tensor_c)\n",
    "print(random_tensor_d)\n",
    "print(random_tensor_c==random_tensor_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "017e2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
      "        0.1332, 0.9346, 0.5936, 0.8694, 0.5677, 0.7411])\n",
      "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
      "        0.1332, 0.9346, 0.5936, 0.8694, 0.5677, 0.7411])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n"
     ]
    }
   ],
   "source": [
    "Random_seed=42\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_c=torch.rand(15)\n",
    "\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_d=torch.rand(15)\n",
    "\n",
    "print(random_tensor_c)\n",
    "print(random_tensor_d)\n",
    "print(random_tensor_c==random_tensor_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f5acb1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2])\n",
      "tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True])\n"
     ]
    }
   ],
   "source": [
    "Random_seed=42\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_c=torch.randint(1,3,   (15,))\n",
    "\n",
    "torch.manual_seed(Random_seed)\n",
    "random_tensor_d=torch.randint(1,3, (15,))\n",
    "\n",
    "print(random_tensor_c)\n",
    "print(random_tensor_d)\n",
    "print(random_tensor_c==random_tensor_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3f1c3c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "randint(low=0, high, size, \\*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "\n",
      "Returns a tensor filled with random integers generated uniformly\n",
      "between :attr:`low` (inclusive) and :attr:`high` (exclusive).\n",
      "\n",
      "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "\n",
      ".. note::\n",
      "    With the global dtype default (``torch.float32``), this function returns\n",
      "    a tensor with dtype ``torch.int64``.\n",
      "\n",
      "Args:\n",
      "    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\n",
      "    high (int): One above the highest integer to be drawn from the distribution.\n",
      "    size (tuple): a tuple defining the shape of the output tensor.\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "    dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``,\n",
      "        this function returns a tensor with dtype ``torch.int64``.\n",
      "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "        Default: ``torch.strided``.\n",
      "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "        Default: if ``None``, uses the current device for the default tensor type\n",
      "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.randint(3, 5, (3,))\n",
      "    tensor([4, 3, 4])\n",
      "\n",
      "\n",
      "    >>> torch.randint(10, (2, 2))\n",
      "    tensor([[0, 2],\n",
      "            [5, 5]])\n",
      "\n",
      "\n",
      "    >>> torch.randint(3, 10, (2, 2))\n",
      "    tensor([[4, 5],\n",
      "            [6, 7]])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "134ae2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_norm = torch.tensor([0.5204, 0.2503, 0.3525, 0.5673, 0.8237, 0.5781, 0.6879, 0.3816, 0.7249, 0.0998,\n",
    "                       0.4321, 0.6543, 0.2356, 0.7890, 0.1287, 0.4562, 0.6723, 0.3456, 0.5671, 0.2345])\n",
    "X1_norm=torch.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d2c62551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5d86e952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "13fdc5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normal_distribution(x: torch.Tensor):  # input should be a tensor\n",
    "    return 1/(torch.sqrt(2*torch.tensor(torch.pi)))*torch.exp(-x**2/2)\n",
    "\n",
    "# Compute PDF values with tensor input\n",
    "y = normal_distribution(X1_norm)\n",
    "\n",
    "# Convert to numpy for matplotlib\n",
    "plt.plot(X1_norm.numpy(), y.numpy(), 'ro')  # 'ro' is red dots\n",
    "plt.title(\"Normal Distribution PDF values at tensor points\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ce821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution(x: float): # normal distribution of mean 0 and std 1\n",
    "    x_tensor = torch.tensor(x)  # convert to tensor\n",
    "    result = 1/(torch.sqrt(2*torch.pi))*torch.exp(-x_tensor**2/2)\n",
    "    return result.item()  # return as a float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e201a",
   "metadata": {},
   "source": [
    "### Putting tensors on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ab5bba40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() #Cuda(GPU) is not available, so it's cpu always, use google collab if you must use a gpu.\n",
    "\n",
    "device='cuda' if torch.cuda.is_available()  else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8be04ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d11b9f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 5]), device(type='cpu'))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Move tensor to GPU if available\n",
    "\n",
    "X=torch.tensor([1,4,5])\n",
    "X_to_device=X.to(device) #no change will happen because there is no GPU\n",
    "X_to_device, X_to_device.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849b348",
   "metadata": {},
   "source": [
    "### numpy does not work with gpu. meaning if you want to transform a tensor into a numpy elt, you must insure that it is on cpu first. X.cpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373077d",
   "metadata": {
    "origin_pos": 111
   },
   "source": [
    "## Summary\n",
    "\n",
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries.\n",
    "Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Run the code in this section. Change the conditional statement `X == Y` to `X < Y` or `X > Y`, and then see what kind of tensor you can get.\n",
    "1. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2776415",
   "metadata": {
    "origin_pos": 113,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/27)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
